# Hashing-as-a-Preprocessing-for-deep-neural-networks
Neural networks have been one of the most beautiful programming paradigms in the world that has simplified various tasks. The cutting edge technologies apply neural network models to achieve their desired targets. Due to a humungous amount of data available, it has been observed that the training and testing phase of neural networks take a longer time. Dimensionality reduction techniques help in compressing the data to reduce the computational efforts required to train the neural network without compromising on the performance of them. In this project, we deal with three types of hashing pipelines that helps in linear mapping of data to a lower dimensional space by giving appropriate weightage to the parts that are critical in defining the component and in performing extensive computation. These hashing pipelines are known to preserve the euclidean distance of vectors in a given space. The three hashing pipelines are Johnson LindenStrauss Transformation, Hadamard Transformations and Principal Component analysis. We are given the MNIST dataset to which the hashing pipelines are applied to generate a reduced matrix that preserves the details of the images which is applied to the neural networks to perform the classification and the stability of these algorithms is studied with respect to the classification accuracy of the neural network. The critical factors such as time complexity, computational efforts and accuracy are studied for these hashing pipelines to understand the performance of the hashing pipelines.
